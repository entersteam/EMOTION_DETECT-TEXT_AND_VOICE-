{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_4_data = pd.read_csv(\"data/4차년도.csv\", encoding=\"cp949\")\n",
    "year_5_data = pd.read_csv(\"data/5차년도.csv\", encoding=\"cp949\")\n",
    "year_5_data_2 = pd.read_csv(\"data/5차년도_2차.csv\", encoding=\"cp949\")\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 14606, 5_1 : 10011, 5_2 : 19374\n"
     ]
    }
   ],
   "source": [
    "print(f\"4 : {len(year_4_data)}, 5_1 : {len(year_5_data)}, 5_2 : {len(year_5_data_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43991\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((year_4_data,year_5_data,year_5_data_2))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0, 'sad': 1, 'fear': 2, 'disgust': 3, 'neutral': 4, 'happiness': 5, 'sadness': 6, 'angry': 7, 'surprise': 8}\n"
     ]
    }
   ],
   "source": [
    "# 기쁨 : 0, 슬픔 : 1, 화남 : 2, 두려움 : 3, 역겨움 : 4, 중립 : 5, 놀라움 : 6\n",
    "emot_indexing_dic = {'happiness' : 0, 'sadness' : 1, 'angry' : 2, 'fear' : 3, 'disgust' : 4, 'neutral' : 5, 'surprise' : 6}\n",
    "emotion_dic = {}\n",
    "for i, emot in enumerate(data['상황'].unique()):\n",
    "    emotion_dic[emot] = i\n",
    "    \n",
    "print(emotion_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = []\n",
    "y = []\n",
    "for i in range(len(data)):\n",
    "    emot = [0]*7\n",
    "    line = data.iloc[i]\n",
    "    x_origin.append(line['발화문'])\n",
    "    for j in range(1,6):\n",
    "        emot[emot_indexing_dic[line[f'{j}번 감정'].lower()]] += line[f'{j}번 감정세기']\n",
    "    y.append(emot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in x_origin:\n",
    "    x.append(okt.morphs(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = 10000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=word_num)\n",
    "tokenizer.fit_on_texts(x)\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "padded_x = pad_sequences(x, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emotion_y = np.array([emotion_dic[emot] for emot in data['상황']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_x, emotion_y, test_size=0.3, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = padded_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43991,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(20000, 300, input_length=maxlen), \n",
    "  tf.keras.layers.LSTM(units=50), \n",
    "  tf.keras.layers.Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "385/385 [==============================] - 32s 78ms/step - loss: 2.0594 - accuracy: 0.2521 - val_loss: 2.0460 - val_accuracy: 0.2569\n",
      "Epoch 2/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 2.0549 - accuracy: 0.2528 - val_loss: 2.0480 - val_accuracy: 0.2570\n",
      "Epoch 3/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 2.0546 - accuracy: 0.2528 - val_loss: 2.0463 - val_accuracy: 0.2569\n",
      "Epoch 4/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 2.0545 - accuracy: 0.2528 - val_loss: 2.0457 - val_accuracy: 0.2570\n",
      "Epoch 5/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 2.0543 - accuracy: 0.2529 - val_loss: 2.0472 - val_accuracy: 0.2570\n",
      "Epoch 6/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 1.9411 - accuracy: 0.2720 - val_loss: 1.6569 - val_accuracy: 0.3233\n",
      "Epoch 7/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 1.5035 - accuracy: 0.4256 - val_loss: 1.3414 - val_accuracy: 0.4973\n",
      "Epoch 8/50\n",
      "385/385 [==============================] - 30s 79ms/step - loss: 1.2053 - accuracy: 0.5447 - val_loss: 1.0877 - val_accuracy: 0.6046\n",
      "Epoch 9/50\n",
      "385/385 [==============================] - 31s 79ms/step - loss: 0.8325 - accuracy: 0.7192 - val_loss: 0.8489 - val_accuracy: 0.7318\n",
      "Epoch 10/50\n",
      "385/385 [==============================] - 31s 80ms/step - loss: 0.5845 - accuracy: 0.7987 - val_loss: 0.7624 - val_accuracy: 0.7607\n",
      "Epoch 11/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.4037 - accuracy: 0.8653 - val_loss: 0.5917 - val_accuracy: 0.8318\n",
      "Epoch 12/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.2781 - accuracy: 0.9156 - val_loss: 0.5584 - val_accuracy: 0.8435\n",
      "Epoch 13/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.2206 - accuracy: 0.9348 - val_loss: 0.5608 - val_accuracy: 0.8500\n",
      "Epoch 14/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.1929 - accuracy: 0.9420 - val_loss: 0.5618 - val_accuracy: 0.8445\n",
      "Epoch 15/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.1694 - accuracy: 0.9495 - val_loss: 0.6173 - val_accuracy: 0.8526\n",
      "Epoch 16/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.1541 - accuracy: 0.9523 - val_loss: 0.6032 - val_accuracy: 0.8514\n",
      "Epoch 17/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.1461 - accuracy: 0.9564 - val_loss: 0.6109 - val_accuracy: 0.8497\n",
      "Epoch 18/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.1335 - accuracy: 0.9583 - val_loss: 0.6394 - val_accuracy: 0.8545\n",
      "Epoch 19/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.1216 - accuracy: 0.9627 - val_loss: 0.6024 - val_accuracy: 0.8522\n",
      "Epoch 20/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.1162 - accuracy: 0.9643 - val_loss: 0.6595 - val_accuracy: 0.8526\n",
      "Epoch 21/50\n",
      "385/385 [==============================] - 30s 78ms/step - loss: 0.1102 - accuracy: 0.9647 - val_loss: 0.6191 - val_accuracy: 0.8505\n",
      "Epoch 22/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.1019 - accuracy: 0.9678 - val_loss: 0.6878 - val_accuracy: 0.8550\n",
      "Epoch 23/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0997 - accuracy: 0.9691 - val_loss: 0.6632 - val_accuracy: 0.8534\n",
      "Epoch 24/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0926 - accuracy: 0.9697 - val_loss: 0.6763 - val_accuracy: 0.8560\n",
      "Epoch 25/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0863 - accuracy: 0.9734 - val_loss: 0.7417 - val_accuracy: 0.8527\n",
      "Epoch 26/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0842 - accuracy: 0.9728 - val_loss: 0.7463 - val_accuracy: 0.8550\n",
      "Epoch 27/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0797 - accuracy: 0.9749 - val_loss: 0.7205 - val_accuracy: 0.8555\n",
      "Epoch 28/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0737 - accuracy: 0.9765 - val_loss: 0.7563 - val_accuracy: 0.8531\n",
      "Epoch 29/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0703 - accuracy: 0.9771 - val_loss: 0.7188 - val_accuracy: 0.8544\n",
      "Epoch 30/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.7843 - val_accuracy: 0.8548\n",
      "Epoch 31/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0658 - accuracy: 0.9788 - val_loss: 0.8130 - val_accuracy: 0.8560\n",
      "Epoch 32/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.7950 - val_accuracy: 0.8508\n",
      "Epoch 33/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0586 - accuracy: 0.9811 - val_loss: 0.8309 - val_accuracy: 0.8532\n",
      "Epoch 34/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0561 - accuracy: 0.9812 - val_loss: 0.8577 - val_accuracy: 0.8555\n",
      "Epoch 35/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0503 - accuracy: 0.9836 - val_loss: 0.8506 - val_accuracy: 0.8529\n",
      "Epoch 36/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0546 - accuracy: 0.9817 - val_loss: 0.7854 - val_accuracy: 0.8513\n",
      "Epoch 37/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0575 - accuracy: 0.9812 - val_loss: 0.7918 - val_accuracy: 0.8550\n",
      "Epoch 38/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0454 - accuracy: 0.9844 - val_loss: 0.8680 - val_accuracy: 0.8539\n",
      "Epoch 39/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0437 - accuracy: 0.9851 - val_loss: 0.8658 - val_accuracy: 0.8537\n",
      "Epoch 40/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0420 - accuracy: 0.9860 - val_loss: 0.9102 - val_accuracy: 0.8474\n",
      "Epoch 41/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0421 - accuracy: 0.9856 - val_loss: 0.8781 - val_accuracy: 0.8474\n",
      "Epoch 42/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0393 - accuracy: 0.9864 - val_loss: 0.8925 - val_accuracy: 0.8493\n",
      "Epoch 43/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 0.9392 - val_accuracy: 0.8484\n",
      "Epoch 44/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0414 - accuracy: 0.9857 - val_loss: 0.8683 - val_accuracy: 0.8485\n",
      "Epoch 45/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.9550 - val_accuracy: 0.8461\n",
      "Epoch 46/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0338 - accuracy: 0.9882 - val_loss: 0.8982 - val_accuracy: 0.8479\n",
      "Epoch 47/50\n",
      "385/385 [==============================] - 29s 76ms/step - loss: 0.0314 - accuracy: 0.9889 - val_loss: 0.9997 - val_accuracy: 0.8467\n",
      "Epoch 48/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0331 - accuracy: 0.9888 - val_loss: 0.9810 - val_accuracy: 0.8497\n",
      "Epoch 49/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0349 - accuracy: 0.9870 - val_loss: 0.9851 - val_accuracy: 0.8516\n",
      "Epoch 50/50\n",
      "385/385 [==============================] - 30s 77ms/step - loss: 0.0281 - accuracy: 0.9902 - val_loss: 1.0320 - val_accuracy: 0.8516\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 나는 사과만 보면 너무 좋아\n",
      "2 [['나', '는', '사과', '만', '보면', '너무', '좋아']]\n",
      "3 [[5, 18, 397, 54, 633, 7, 233]]\n",
      "4 [[  5  18 397  54 633   7 233   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "test_sentence = '나는 사과만 보면 너무 좋아'\n",
    "print(1,test_sentence)\n",
    "test_sentence = [okt.morphs(test_sentence)]\n",
    "print(2,test_sentence)\n",
    "\n",
    "\n",
    "test_sentence = tokenizer.texts_to_sequences(test_sentence)\n",
    "print(3,test_sentence)\n",
    "test_sentence = pad_sequences(test_sentence, padding='post', maxlen=maxlen)\n",
    "print(4,test_sentence)\n",
    "\n",
    "print(np.argmax(model.predict(test_sentence)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
